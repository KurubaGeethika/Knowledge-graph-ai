# -*- coding: utf-8 -*-
"""Copy of AIGurukul.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z-wYJq3qstqPChc1whrj-9ocVACCE766

Load JSON data â†’ convert to DataFrame

Clean / prepare data

Generate embeddings for prompt + response

Store in a vector database (FAISS) with metadata

RAG: retrieve top-K similar prompts for a user query

Use open-source LLM for generating an answer

Environment setup
"""

!pip install pandas faiss-cpu sentence-transformers transformers accelerate

"""Importing packages"""

import pandas as pd
import numpy as np
import json
from sentence_transformers import SentenceTransformer
import faiss
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

"""STEP 1: Load JSON data"""

from google.colab import files
import io

uploaded = files.upload()  # This will prompt you to select a file

# Get the uploaded filename
filename = list(uploaded.keys())[0]
print("Uploaded file:", filename)

import json
import pandas as pd
with open(filename, 'r') as f:
    data = json.load(f)

# Convert to DataFrame
df = pd.DataFrame(data)
print("Initial DataFrame shape:", df.shape)
df.head()

"""STEP 2 : Basic cleaning"""

# Initial number of rows
initial_rows = len(df)
print(f"Initial number of rows: {initial_rows}")

# Track missing values before filling
print("Missing values before filling:")
print(df.isna().sum())

# Fill missing/null values
df['questionType'] = df['questionType'].fillna("Unknown questionType")
df['answerTime'] = df['answerTime'].fillna("Unknown answerTime")
df['unixTime'] = df['unixTime'].fillna("Unknown unixTime")
df['answerType'] = df['answerType'].fillna("Unknown answerType")
df['answer'] = df['answer'].fillna("No answer generated")

# Track missing values after filling
print("\nMissing values after filling:")
print(df.isna().sum())

# Drop rows where critical fields are missing
before_drop = len(df)
df = df.dropna(subset=['question', 'asin'])
after_drop = len(df)
print(f"\nRows dropped: {before_drop - after_drop}")
print(f"Remaining rows: {after_drop}")

# Concatenate prompt + response for embedding
df['text_for_embedding'] = df['question'] + " " + df['answer']
df.head()

# Optional: add length or other features
df['text_length'] = df['text_for_embedding'].apply(len)
df.head()

"""STEP 3 : Generate Embeddings"""

!pip install -U sentence-transformers

import torch
from sentence_transformers import SentenceTransformer

# Use a SentenceTransformer model (GPU supported)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
embed_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)

# Generate embeddings
embeddings = embed_model.encode(df['text_for_embedding'].tolist(), convert_to_numpy=True, show_progress_bar=True)
print("Embeddings shape:", embeddings.shape)

"""STEP 4: Create FAISS vector DB"""

# Install the CPU version (recommended for most tasks)
!pip install faiss-cpu

# Import the library
import faiss

d = embeddings.shape[1]  # dimension of embeddings
index = faiss.IndexFlatL2(d)  # L2 distance; for cosine, normalize first
faiss.normalize_L2(embeddings)  # normalize for cosine similarity
index.add(embeddings)
print("Vector DB size:", index.ntotal)

"""STEP 5: RAG Retrieval"""

def retrieve_top_k(query, k=5):
    query_vec = embed_model.encode([query], convert_to_numpy=True)
    faiss.normalize_L2(query_vec)
    D, I = index.search(query_vec, k)
    results = df.iloc[I[0]]
    return results

# Example query
user_question = "Can I use 2 SIMs on iPhone 14 in the US?"
top_k_records = retrieve_top_k(user_question, k=5)
print("Top-k retrieved records:\n", top_k_records[['question', 'answer']])

"""# STEP 6: Generate LLM answer"""

# Build context from retrieved records
context = ""
for idx, row in top_k_records.iterrows():
    context += f"Q: {row['question']}\nA: {row['answer']}\n\n"

rag_prompt = f"""
You are a smartphone assistant. Use the following Q&A to answer the user's question.

User Question: {user_question}

Context:
{context}

Answer concisely:
"""

!pip install transformers

from transformers import AutoTokenizer

from transformers import AutoModelForCausalLM

# Use a small open-source LLM (CPU/GPU)
# llm_model = "TheBloke/vicuna-7B-1.1-HF"  # or a smaller model if GPU is limited
# tokenizer = AutoTokenizer.from_pretrained(llm_model)
# model = AutoModelForCausalLM.from_pretrained(llm_model, device_map='auto', torch_dtype=torch.float16)

# inputs = tokenizer(rag_prompt, return_tensors="pt").to(device)
# outputs = model.generate(**inputs, max_new_tokens=200)
# answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

# print("\n=== Generated Answer ===")
# print(answer)

"""Save"""

!git --version
!pip install PyGithub